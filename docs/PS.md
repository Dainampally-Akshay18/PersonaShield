# üß† First ‚Äî What problem are they ACTUALLY talking about?

Not:

> People share personal data online

That‚Äôs obvious.

The real problem is:

> **Privacy damage doesn‚Äôt come from one leak ‚Äî it comes from connecting multiple harmless facts.**

This is called **correlation attack**.

---

## Real-life example

Imagine your resume says:

* B.Tech 2021
* Lives in Bangalore
* Backend developer at Infosys

Your LinkedIn says:

* Celebrated 25th birthday üéâ

Your GitHub username:

* akshay99

None of these are sensitive alone.

But an attacker combines:

```
Graduation year ‚Üí age range
Birthday post ‚Üí exact age
Username 99 ‚Üí birth year confirmation
Company ‚Üí believable email context
City ‚Üí local targeting
```

Now attacker knows:

* age
* workplace
* routine
* believable story

And sends:

> ‚ÄúInfosys payroll update for 2021 batch employees ‚Äî verify immediately‚Äù

You click.

---

üëâ This is the problem statement.

Not data leakage.

**Inference leakage**

---

# Why Existing Privacy Tools Fail

Current tools do:

> detect phone number
> detect email
> mask Aadhaar

But attackers don‚Äôt need Aadhaar.

They need:

> believable context

So current tools protect identity
But attackers exploit psychology

---

# What The Hackathon Wants You To Build

You are NOT building a privacy checker.

You are building:

> A simulator of how attackers think

The system must answer:

> ‚ÄúIf I were a hacker, what can I figure out about you?‚Äù

---

# Breaking Down the Problem Statement Line by Line

---

## ‚Äúshare fragments of personal data‚Äù

Meaning:

User data exists in pieces across platforms.

Not centralized.

So risk comes from aggregation.

---

## ‚Äúharmless in isolation‚Äù

Important.

If one platform leaks ‚Üí low danger
If many combine ‚Üí high danger

You must evaluate combinations.

---

## ‚Äúattackers can correlate multiple data points‚Äù

This is the heart of the challenge.

You must build a system that does:

Human reasoning:

```
A + B ‚Üí inference
B + C ‚Üí stronger inference
A + B + C ‚Üí attack opportunity
```

---

## ‚ÄúCurrent tools detect PII‚Äù

They check:

* email
* phone
* SSN

But they do NOT check:

> Can someone convincingly impersonate you?

Your system must.

---

## ‚Äúsimulate adversarial thinking‚Äù

This is the most important phrase.

They don‚Äôt want classification AI.

They want:

> AI that behaves like an attacker

---

## ‚Äúcompound privacy exposure risk‚Äù

Compound = combined effect

Example:

| Data         | Risk |
| ------------ | ---- |
| Email        | low  |
| Company      | low  |
| Role         | low  |
| All together | HIGH |

You measure the combined risk.

---

# What Your System Must Do (Simplified)

1. Read user data
2. Extract facts
3. Connect facts
4. See what attacker learns
5. Measure attack possibility
6. Show how they get attacked

---

# Final Simple Explanation (You can say this in presentation)

> ‚ÄúPrivacy is no longer about secret data ‚Äî it's about predictable identity.
> Attackers don‚Äôt steal information, they reconstruct people.
> Our system reconstructs the user the same way an attacker would, and measures how dangerous that reconstruction is.‚Äù

---

Now the problem statement should feel very clear:

You are building a **human-intelligence simulator**, not a scanner.

